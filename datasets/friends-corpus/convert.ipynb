{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Corpus, User, Utterance\n",
    "import json\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each season of _Friends_ is presented in json format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached https://files.pythonhosted.org/packages/95/9c/afd55bb35cc03e4b3dadc41dd48bc26e0678b08d59f32411735c35bda550/spacy-2.1.8-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from spacy) (0.2.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from spacy) (0.1.0)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from spacy) (1.16.2)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from spacy) (7.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from spacy) (0.2.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.10.15)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.23)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.26.0)\n",
      "Installing collected packages: spacy\n",
      "Successfully installed spacy-2.1.8\n",
      "\u001b[33mWARNING: You are using pip version 19.2.2, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#you may need this:#\n",
    "#!pip install spacy#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = ['friends_season_01.json',\n",
    "                  'friends_season_02.json',\n",
    "                  'friends_season_03.json',\n",
    "                  'friends_season_04.json',\n",
    "                  'friends_season_05.json',\n",
    "                  'friends_season_06.json',\n",
    "                  'friends_season_07.json',\n",
    "                  'friends_season_08.json',\n",
    "                  'friends_season_09.json',\n",
    "                  'friends_season_10.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the purposes of the notebook, here's a means of dowloading the corpus virtually.#\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# read the JSON file from the web\n",
    "for file in input_files:\n",
    "    link = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/' + file\n",
    "    r = requests.get(link)\n",
    "\n",
    "#loading as seasons#\n",
    "season = json.loads(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USER CORPUS**\n",
    "Developing a robust user corpus is a priority of my project given its focus on characterization. Knowing this, I spent the bulk of my working generating a few additional types of metadata to include with each user. Another priority of the code is to retain season-level information within the grander context of the series as a whole.\n",
    "\n",
    "In future versions of this dataset, I would be interested in including episode-level and scene-level information about users. In terms of sourcing outside metadata, I think that Gender metadata could be sourced from crosslisting character names with IMDB.\n",
    "\n",
    "Below, I offer an example of the current pipeline to give an overview of the conversion process. This pipeline  Since two definitions have not been defined yet, it is unfunctional as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_distribution = []\n",
    "for name in input_files:\n",
    "    with open(name) as data:\n",
    "        season = json.load(data)\n",
    "        season_id = season['season_id']\n",
    "        episodes = season['episodes']\n",
    "        character_distribution.append([season_id,season_speaking_users(episodes)])\n",
    "character_matrix = series_speaking_users(character_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline above stresses the production of a season-level _character distribution_ and a series-level _character matrix_. I divide the dataset as such in order to better account for the distribution of user dialogue and reference across different scales of narrative. The function **season_speaking_users** works to divide and count users into two main roles, speakers and figures of reference, and assign the quality of being a nonspeaker, a nonspeaking user who is referenced. Nonspeaking users fascinatingly are typically either famous guest stars (like Ed Begley Jr.) or entirely generic onscreen figures, like a silent airplane steward.\n",
    "\n",
    "While seasons one through four feature an additional category called \"character_entities\" that refers to the characters mentioned in or around the conversation, this convention is dropped from season five on. That is, it's not possible using the current dataset to track nonspeaking users throughout the duration of the season. I do believe, however, it would be worthwhile to see if: 1.)nonspeaking characters reccur or become a type of trope (Are there silent characters who show up on screen and shrug for laugh? Is it usually poorly-disguised celebrities who fill these roles to produce a sight gag?) 2.)If the characaters who are referenced by main characters in earlier seasons end up getting more speaking roles as the series progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def season_speaking_users(episodes):\n",
    "    ssu = []\n",
    "    sru = []\n",
    "    for id in range(len(episodes)):\n",
    "        episode = episodes[id]\n",
    "        scenes = episode['scenes']\n",
    "        for scene in scenes:\n",
    "            for place in range(len((scene['utterances']))):\n",
    "                utterance = scene['utterances'][place]\n",
    "                speakers = utterance['speakers']\n",
    "                ##1.) See markup below##\n",
    "                if len(speakers) >= 1:\n",
    "                    for p in range(len(speakers)):\n",
    "                        ssu.append(speakers[p])\n",
    "                if 'character_entities' in utterance:\n",
    "                    character_entities = utterance['character_entities']\n",
    "                    for place in range(len(character_entities)):\n",
    "                        if len(character_entities[place]) >1:\n",
    "                            character_range = character_entities[place]\n",
    "                            for position in range(len(character_range)):\n",
    "                                sru.append(character_range[position][2])\n",
    "                \n",
    "                \n",
    "    ##counting##\n",
    "    season_speaking_users = Counter(ssu)\n",
    "    season_referenced_users = Counter(sru)\n",
    "    season_non_speaking_users = []\n",
    "    \n",
    "    ##finding non-speaking users##\n",
    "    for key in [i for i in season_referenced_users]:\n",
    "        if key in [i for i in season_speaking_users]: \n",
    "            pass\n",
    "        else:\n",
    "            season_non_speaking_users.append(key)\n",
    "    return [season_speaking_users, season_referenced_users, season_non_speaking_users]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above uses counters to measure the number of utterance and references each character makes. It also features a small amount of code to discover nonspeaking users. Ultimately, the code produces two dictionaries that take the names of characters for keys and return the number of utterances and references that respectively occur within the season. It also returns a list of nonspeaking users.\n",
    "\n",
    "1.) One of the larger peculiarities of the dataset was instances where characters would talk in unison. I'm not sure if there's a good way to parse this - should the collective be treated as a single user? - though it would be interesting to see if there are certain combinations that happen frequently throughout the seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_speaking_users(character_distribution):\n",
    "   \n",
    "    ## 1.##\n",
    "    all_characters =[]\n",
    "    for season in character_distribution:\n",
    "        speakers = season[1][0]\n",
    "        referees = season[1][1]\n",
    "        nonspeakers = season[1][2]\n",
    "        for speaker in speakers:\n",
    "            if speaker not in all_characters:\n",
    "                all_characters.append(speaker)\n",
    "        for referee in referees:\n",
    "            if referee not in all_characters:\n",
    "                all_characters.append(referee)\n",
    "        for ns in nonspeakers:\n",
    "            if ns not in all_characters:\n",
    "                    all_characters.append(ns)\n",
    "    ## 2. ##                \n",
    "    characters_tagged = dict()\n",
    "    for character in all_characters:\n",
    "        total_spoken = 0\n",
    "        season_spoken = []\n",
    "        total_referenced = 0\n",
    "        season_referenced = []\n",
    "        for season in character_distribution:\n",
    "            speakers = season[1][0]\n",
    "            referees = season[1][1]\n",
    "            season_id = season[0]\n",
    "            total_spoken += speakers[character]\n",
    "            season_spoken.append([season_id , speakers[character]])\n",
    "            total_referenced += referees[character]\n",
    "            season_referenced.append([season_id, referees[character]])\n",
    "        characters_tagged[character] = [total_spoken, season_spoken, total_referenced, season_referenced]\n",
    "    return characters_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.) In order to collect series-level statistics from the season-level this function's first half collects the names of all users across the two qualities and nonspeaking condition.\n",
    "\n",
    "2.) That list is then fed into and combined with the season-level dictionaries created by the previous function. Two **total** qualities track all of the utterances and references that occur for a user within the series, while season_spoken and season_reference return each season and its counts. Part of the intuition for this approach is to see how much a character's share and quantity of dialogue changes through the course of several seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_distribution = []\n",
    "for file in input_files:\n",
    "    link = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/' + file\n",
    "    r = requests.get(link)\n",
    "    season = json.loads(r.text)\n",
    "    season_id = season['season_id']\n",
    "    episodes = season['episodes']\n",
    "    character_distribution.append([season_id,season_speaking_users(episodes)])\n",
    "character_matrix = series_speaking_users(character_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example of how the character_matrix works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9417,\n",
       " [['s01', 865],\n",
       "  ['s02', 819],\n",
       "  ['s03', 936],\n",
       "  ['s04', 980],\n",
       "  ['s05', 921],\n",
       "  ['s06', 1002],\n",
       "  ['s07', 1107],\n",
       "  ['s08', 1089],\n",
       "  ['s09', 913],\n",
       "  ['s10', 785]],\n",
       " 4068,\n",
       " [['s01', 905],\n",
       "  ['s02', 982],\n",
       "  ['s03', 1096],\n",
       "  ['s04', 1085],\n",
       "  ['s05', 0],\n",
       "  ['s06', 0],\n",
       "  ['s07', 0],\n",
       "  ['s08', 0],\n",
       "  ['s09', 0],\n",
       "  ['s10', 0]]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_matrix['Rachel Green']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_meta = {}\n",
    "for user in character_matrix.keys():\n",
    "    user_meta[user] =   {\"character_name\": character_matrix[user],\n",
    "                               \"total_utterances\": character_matrix[user][0],\n",
    "                               \"utterances_per_season\": character_matrix[user][1],\n",
    "                               \"total_references\": character_matrix[user][2],\n",
    "                               \"references_per_season\": character_matrix[user][3]}\n",
    "                \n",
    "\n",
    "##making Corpus##\n",
    "corpus_users = {k: User(name = k, meta = v) for k,v in user_meta.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version of the User Corpus, each user ends up with five characteristics. I intend to use the proportions of these characteristics to determine what role utterance and reference quantity play in separating major and minor characters across seasons.\n",
    "\n",
    "In terms of adding future metadata, I would be interested in seeing the average-sized converation each character participate in per season and the average size of the groups those conversations take place in. When thinking about major and minor characters, it would be curious to see if speaking-but-not-major characters tend to be relegated to certain group sizes or shorter conversational instances.\n",
    "\n",
    "What I'd love most in terms of metadata, however, was a way to parse the \"transcript with note\" subcategory in each utterance for character information/set directions. It would be fascinating to see which physical behaviors and details get assigned to which character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UTTERANCE CORPUS**\n",
    "Give my project's interest in users, I've made significantly fewer modifications in generating metadata for the utterance corpus. Nevertheless, I believe that in future iterations of this code, the utterance corpus will make some of the user-processing accomodations I made above redundant. So it goes with version 1!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_corpus = {}\n",
    "for file in input_files:\n",
    "    link = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/' + file\n",
    "    r = requests.get(link)\n",
    "    season = json.loads(r.text)\n",
    "    season_id = season['season_id']\n",
    "    episodes = season['episodes']\n",
    "    #############################\n",
    "    for id in range(len(episodes)):\n",
    "        episode = episodes[id]\n",
    "        scenes = episode['scenes']\n",
    "        for scene in scenes:\n",
    "            for place in range(len((scene['utterances']))):\n",
    "                utterance = scene['utterances'][place]\n",
    "                id = utterance['utterance_id']\n",
    "\n",
    "                ##1.)Some utterances involve multiple speakers stored as a list. This process treats them individually, though it is worth##\n",
    "                ##contesting whether two characters speaking in unison is a singular utterance##\n",
    "                if len(utterance[\"speakers\"]) >= 1:\n",
    "                    for p in range(len(utterance[\"speakers\"])):\n",
    "                        user = User(utterance[\"speakers\"][p])\n",
    "\n",
    "                ##2.)Getting the root is relatively easy given how organized the dataset is already. This code replaces the ending of scenes##\n",
    "                ##with the first utterance of the scene##\n",
    "                part = id.split('_')[:3]\n",
    "                part.append('u001')\n",
    "                root = '_'.join(part)\n",
    "\n",
    "                ##3.)There is a 'character_entities' subsection of each utterance that features all characters involved and or referenced.##\n",
    "                ##Designating a reply from this data is theoretically more accurate than going to the previous utterance, but not all seasons##\n",
    "                ##contain this metadata##\n",
    "                if id.split('_')[3] == \"u001\":\n",
    "                    reply_to = None\n",
    "                else:\n",
    "                    prior = scene['utterances'][place - 1]['speakers']\n",
    "                    if len(prior) >= 1:\n",
    "                        for p in range(len(prior)):\n",
    "                            reply_to = prior[p]\n",
    "                    else:\n",
    "                        reply_to = None\n",
    "                timestamp = None\n",
    "\n",
    "                ##there's a tokenized version available in the data set too##\n",
    "                text = utterance[\"transcript\"]\n",
    "                utterance_corpus[id] = Utterance(id, user, root, reply_to, timestamp, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To expand on the comments above:\n",
    "\n",
    "2.) In thinking more about the qualities of syndicated televison, many scenes and conversations begin in medias res or correspond to a cliffhanger from before the commercial break. I can't think of a better to determine where a conversation starts, but I do think it's important to measure the composition and similarity of conversations across scenes.\n",
    "\n",
    "3.) In measuring replies, the code-as-is assumes that the current utterance is responding to the one immediately before it. Honestly, I don't feel super comfortable with this assumption. For one, _Friends_ is known for having at least one two characters who introduce nonsequitirs into conversation. **A distinction should be made between if being a part of a scene and being a part of a conversation are the same thing, especially in a comedy**. I would be interested to see how well the character_entities data corresponds to assuming conversationality in a linear representation of a scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_list = [utterance for k,utterance in utterance_corpus.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_corpus = Corpus(utterances=utterance_list, version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_ids = series_corpus.get_conversation_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the three bits of code from the tutorial. Since not much worthwhile, additional metadata exits to add to this corpus, I have forgone it. If I was able to link IMDB to characters, however, I would also be able to get average ratings for each episode. This coudl be interesting in seeing if there's a generally positive response to certain cohorts of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of conversations in the dataset = 3107\n"
     ]
    }
   ],
   "source": [
    "print(\"number of conversations in the dataset = {}\".format(len(series_corpus.get_conversation_ids())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_corpus.dump(\"friends_corpus\", base_path= \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'posixpath' from '/home/nbuser/anaconda3_501/lib/python3.6/posixpath.py'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users: 699\n",
      "Number of Utterances: 67373\n",
      "Number of Conversations: 3107\n"
     ]
    }
   ],
   "source": [
    "series_corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
