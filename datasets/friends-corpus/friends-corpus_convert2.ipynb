{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#you may need this:#\n!pip install spacy",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#And this#\n!python -m spacy download en",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#One more#\n!pip install nltk",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from convokit import Corpus, User, Utterance, Prominence\nimport json\nfrom collections import Counter",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "input_files = ['friends_season_01.json',\n                  'friends_season_02.json',\n                  'friends_season_03.json',\n                  'friends_season_04.json',\n                  'friends_season_05.json',\n                  'friends_season_06.json',\n                  'friends_season_07.json',\n                  'friends_season_08.json',\n                  'friends_season_09.json',\n                  'friends_season_10.json']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#for the purposes of the notebook, here's a means of dowloading the corpus virtually.#\nimport requests\nimport json\n\n# read the JSON file from the web\nfor file in input_files:\n    link = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/' + file\n    r = requests.get(link)\n\n#loading as seasons#\nseason = json.loads(r.text)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**USER CORPUS**\nDeveloping a robust user corpus is a priority of my project given its focus on characterization. Knowing this, I spent the bulk of my working generating a few additional types of metadata to include with each user. Another priority of the code is to retain season-level information within the grander context of the series as a whole.\n\nIn future versions of this dataset, I would be interested in including episode-level and scene-level information about users. In terms of sourcing outside metadata, I think that Gender metadata could be sourced from crosslisting character names with IMDB.\n\nBelow, I offer an example of the current pipeline to give an overview of the conversion process. This pipeline  Since two definitions have not been defined yet, it is unfunctional as is."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "character_distribution = []\nfor name in input_files:\n    with open(name) as data:\n        season = json.load(data)\n        season_id = season['season_id']\n        episodes = season['episodes']\n        character_distribution.append([season_id,season_speaking_users(episodes)])\ncharacter_matrix = series_speaking_users(character_distribution)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The pipeline above stresses the production of a season-level _character distribution_ and a series-level _character matrix_. I divide the dataset as such in order to better account for the distribution of user dialogue and reference across different scales of narrative. The function **season_speaking_users** works to divide and count users into two main roles, speakers and figures of reference, and assign the quality of being a nonspeaker, a nonspeaking user who is referenced. Nonspeaking users fascinatingly are typically either famous guest stars (like Ed Begley Jr.) or entirely generic onscreen figures, like a silent airplane steward.\n\nWhile seasons one through four feature an additional category called \"character_entities\" that refers to the characters mentioned in or around the conversation, this convention is dropped from season five on. That is, it's not possible using the current dataset to track nonspeaking users throughout the duration of the season. I do believe, however, it would be worthwhile to see if: 1.)nonspeaking characters reccur or become a type of trope (Are there silent characters who show up on screen and shrug for laugh? Is it usually poorly-disguised celebrities who fill these roles to produce a sight gag?) 2.)If the characaters who are referenced by main characters in earlier seasons end up getting more speaking roles as the series progresses."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def season_speaking_users(episodes):\n    ssu = []\n    sru = []\n    for id in range(len(episodes)):\n        episode = episodes[id]\n        scenes = episode['scenes']\n        for scene in scenes:\n            for place in range(len((scene['utterances']))):\n                utterance = scene['utterances'][place]\n                speakers = utterance['speakers']\n                ##1.) See markup below##\n                if len(speakers) >= 1:\n                    for p in range(len(speakers)):\n                        ssu.append(speakers[p])\n                if 'character_entities' in utterance:\n                    character_entities = utterance['character_entities']\n                    for place in range(len(character_entities)):\n                        if len(character_entities[place]) >1:\n                            character_range = character_entities[place]\n                            for position in range(len(character_range)):\n                                sru.append(character_range[position][2])\n                \n                \n    ##counting##\n    season_speaking_users = Counter(ssu)\n    season_referenced_users = Counter(sru)\n    season_non_speaking_users = []\n    \n    ##finding non-speaking users##\n    for key in [i for i in season_referenced_users]:\n        if key in [i for i in season_speaking_users]: \n            pass\n        else:\n            season_non_speaking_users.append(key)\n    return [season_speaking_users, season_referenced_users, season_non_speaking_users]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The code above uses counters to measure the number of utterance and references each character makes. It also features a small amount of code to discover nonspeaking users. Ultimately, the code produces two dictionaries that take the names of characters for keys and return the number of utterances and references that respectively occur within the season. It also returns a list of nonspeaking users.\n\n1.) One of the larger peculiarities of the dataset was instances where characters would talk in unison. I'm not sure if there's a good way to parse this - should the collective be treated as a single user? - though it would be interesting to see if there are certain combinations that happen frequently throughout the seasons."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def series_speaking_users(character_distribution):\n   \n    ## 1.##\n    all_characters =[]\n    for season in character_distribution:\n        speakers = season[1][0]\n        referees = season[1][1]\n        nonspeakers = season[1][2]\n        for speaker in speakers:\n            if speaker not in all_characters:\n                all_characters.append(speaker)\n        for referee in referees:\n            if referee not in all_characters:\n                all_characters.append(referee)\n        for ns in nonspeakers:\n            if ns not in all_characters:\n                    all_characters.append(ns)\n    ## 2. ##                \n    characters_tagged = dict()\n    for character in all_characters:\n        total_spoken = 0\n        season_spoken = []\n        total_referenced = 0\n        season_referenced = []\n        for season in character_distribution:\n            speakers = season[1][0]\n            referees = season[1][1]\n            season_id = season[0]\n            total_spoken += speakers[character]\n            season_spoken.append([season_id , speakers[character]])\n            total_referenced += referees[character]\n            season_referenced.append([season_id, referees[character]])\n        characters_tagged[character] = [total_spoken, season_spoken, total_referenced, season_referenced]\n    return characters_tagged",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "1.) In order to collect series-level statistics from the season-level this function's first half collects the names of all users across the two qualities and nonspeaking condition.\n\n2.) That list is then fed into and combined with the season-level dictionaries created by the previous function. Two **total** qualities track all of the utterances and references that occur for a user within the series, while season_spoken and season_reference return each season and its counts. Part of the intuition for this approach is to see how much a character's share and quantity of dialogue changes through the course of several seasons."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "character_distribution = []\nfor file in input_files:\n    link = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/' + file\n    r = requests.get(link)\n    season = json.loads(r.text)\n    season_id = season['season_id']\n    episodes = season['episodes']\n    character_distribution.append([season_id,season_speaking_users(episodes)])\ncharacter_matrix = series_speaking_users(character_distribution)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's see an example of how the character_matrix works!"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "character_matrix['Rachel Green']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "user_meta = {}\nfor user in character_matrix.keys():\n    user_meta[user] =   {\"character_name\": character_matrix[user],\n                               \"total_utterances\": character_matrix[user][0],\n                               \"utterances_per_season\": character_matrix[user][1],\n                               \"total_references\": character_matrix[user][2],\n                               \"references_per_season\": character_matrix[user][3]}\n                \n\n##making Corpus##\ncorpus_users = {k: User(name = k, meta = v) for k,v in user_meta.items()}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this version of the User Corpus, each user ends up with five characteristics. I intend to use the proportions of these characteristics to determine what role utterance and reference quantity play in separating major and minor characters across seasons.\n\nIn terms of adding future metadata, I would be interested in seeing the average-sized converation each character participate in per season and the average size of the groups those conversations take place in. When thinking about major and minor characters, it would be curious to see if speaking-but-not-major characters tend to be relegated to certain group sizes or shorter conversational instances.\n\nWhat I'd love most in terms of metadata, however, was a way to parse the \"transcript with note\" subcategory in each utterance for character information/set directions. It would be fascinating to see which physical behaviors and details get assigned to which character."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**UTTERANCE CORPUS**\nGiven my project's interest in users, I've made significantly fewer modifications in generating metadata for the utterance corpus. Nevertheless, I believe that in future iterations of this code, the utterance corpus will make some of the user-processing accomodations I made above redundant. So it goes with version 1!\n\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "utterance_corpus = {}\nfor file in input_files:\n    link = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/' + file\n    r = requests.get(link)\n    season = json.loads(r.text)\n    season_id = season['season_id']\n    episodes = season['episodes']\n    #############################\n    for id in range(len(episodes)):\n        episode = episodes[id]\n        scenes = episode['scenes']\n        for scene in scenes:\n            for place in range(len((scene['utterances']))):\n                utterance = scene['utterances'][place]\n                id = utterance['utterance_id']\n\n                ##1.)Some utterances involve multiple speakers stored as a list. This process treats them individually, though it is worth##\n                ##contesting whether two characters speaking in unison is a singular utterance##\n                if len(utterance[\"speakers\"]) >= 1:\n                    for p in range(len(utterance[\"speakers\"])):\n                        user = User(utterance[\"speakers\"][p])\n\n                ##2.)Getting the root is relatively easy given how organized the dataset is already. This code replaces the ending of scenes##\n                ##with the first utterance of the scene##\n                part = id.split('_')[:3]\n                part.append('u001')\n                root = '_'.join(part)\n\n                ##3.)There is a 'character_entities' subsection of each utterance that features all characters involved and or referenced.##\n                ##Designating a reply from this data is theoretically more accurate than going to the previous utterance, but not all seasons##\n                ##contain this metadata##\n                if id.split('_')[3] == \"u001\":\n                    reply_to = None\n                else:\n                    prior = scene['utterances'][place - 1]['speakers']\n                    if len(prior) >= 1:\n                        for p in range(len(prior)):\n                            reply_to = prior[p]\n                    else:\n                        reply_to = None\n                timestamp = None\n\n                ##there's a tokenized version available in the data set too##\n                text = utterance[\"transcript\"]\n                utterance_corpus[id] = Utterance(id, user, root, reply_to, timestamp, text)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To expand on the comments above:\n\n2.) In thinking more about the qualities of syndicated televison, many scenes and conversations begin in medias res or correspond to a cliffhanger from before the commercial break. I can't think of a better to determine where a conversation starts, but I do think it's important to measure the composition and similarity of conversations across scenes.\n\n3.) In measuring replies, the code-as-is assumes that the current utterance is responding to the one immediately before it. Honestly, I don't feel super comfortable with this assumption. For one, _Friends_ is known for having at least one two characters who introduce nonsequitirs into conversation. **A distinction should be made between if being a part of a scene and being a part of a conversation are the same thing, especially in a comedy**. I would be interested to see how well the character_entities data corresponds to assuming conversationality in a linear representation of a scene."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "utterance_list = [utterance for k,utterance in utterance_corpus.items()]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "series_corpus = Corpus(utterances=utterance_list, version=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "convo_ids = series_corpus.get_conversation_ids()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Above are the three bits of code from the tutorial. Since not much worthwhile, additional metadata exits to add to this corpus, I have forgone it. If I was able to link IMDB to characters, however, I would also be able to get average ratings for each episode. This coudl be interesting in seeing if there's a generally positive response to certain cohorts of users."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"number of conversations in the dataset = {}\".format(len(series_corpus.get_conversation_ids())))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "series_corpus.print_summary_stats()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Transformations and Parsing**\n\nThis section involves parsing conversations in the series corpus. It takes quite a long time."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from convokit import Parser",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "annotator = Parser()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "series_corpus = annotator.fit_transform(series_corpus)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Custom Transformer: User Prominence**\n\nThis transformer separates and tags users into a series of tiers: Major, Major-Aspirational, and Minor. These classifcations rely on five metrics/\n\n1.) Politeness Complexity: In a random sampling of conversations, how varied is the user's politeness.\n2.) Utterances-per-conversation: In a random sampling of conversations, does the user's share of remarks account for more than the number of users/number of utterances per scene.\n3.) First/Last Word: In a random sampling of conversations, how often did the user start or finish a conversation\n4.) Spoken-of: Count of how many users use the user's name in speech\n5.) Raw-count: User's number of utterances/ all utterances.\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Dalliances with Politeness**\n\nIn this section, I explore a few ways that politeness could be correlated with a character's longevity in the show.  Another way of framing this is do characters who have similar compositions of impolite|polite conversation to major characters have a higher likelihood of remaining on the show?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#this might be necessary#\nimport nltk\nnltk.download('punkt')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#this certainly is necessary#\nfrom convokit import PolitenessStrategies\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ps = PolitenessStrategies(verbose=100)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "politeness_corpus = ps.transform(series_corpus)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Prominence Transformer**\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from convokit import Prominence",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pro = Prominence()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pro_corpus = pro.transform(politeness_corpus)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "**k_means Clustering**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "characters = pro_corpus.get_usernames()\nrows = []\nfor character in characters:\n    rows.append(list(pro_corpus.get_user(character).meta.values()))\ncharacter_prom = pd.DataFrame(rows, index= characters)\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "character_prom",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "character_prom[0] = character_prom[0] / character_prom[0].max()\ncharacter_prom[1] = character_prom[1] / character_prom[1].max()\ncharacter_prom[2] = character_prom[2] / character_prom[2].max()\ncharacter_prom[3] = character_prom[3] / character_prom[3].max()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import sklearn",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "mat = character_prom.values\nkm = sklearn.cluster.KMeans(n_clusters=3)\nkm.fit(mat)\nlabels = km.labels_\nresults = pd.DataFrame([character_prom.index,labels]).T",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Metrics**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "to_graph =[]\ncharacters = pro_corpus.get_usernames()\nfor character in characters:\n    user1 = series_corpus.get_user(character)\n    utterances = len(user1.get_utterance_ids())\n    to_graph.append((character, pro_corpus.get_user(character).meta[\"politeness_complexity\"], utterances))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x = [i[1] for i in to_graph]\ny = [i[2] for i in to_graph]\nplt.xlabel('Complexity')\nplt.ylabel('Utterances')\nplt.title('Utterances to Politeness Complexity (All Characters)')\nplt.scatter(x, y)\nplt.savefig('fig1.png', dpi=200)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "to_graph =[]\ncharacters = pro_corpus.get_usernames()\nfor character in characters:\n    user1 = series_corpus.get_user(character)\n    utterances = len(user1.get_utterance_ids())\n    if utterances < 500:\n        to_graph.append((character, pro_corpus.get_user(character).meta[\"politeness_complexity\"], utterances))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x = [i[1] for i in to_graph]\ny = [i[2] for i in to_graph]\nplt.xlabel('Complexity')\nplt.ylabel('Utterances')\nplt.title('Utterances to Politeness Complexity(Major Removed)')\nplt.scatter(x, y)\nplt.savefig('fig2.png', dpi=200)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "to_graph2 = []\ncharacters = pro_corpus.get_usernames()\nfor character in characters:\n    user1 = series_corpus.get_user(character)\n    utterances = len(user1.get_utterance_ids())\n    to_graph2.append((character, pro_corpus.get_user(character).meta[\"utterance_per_conversation\"], utterances))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x = [i[1] for i in to_graph2]\ny = [i[2] for i in to_graph2]\nplt.xlabel('Utterances Per Conversation')\nplt.ylabel('Utterances')\nplt.title('Utterances to Share of Conversation(All Characters)')\nplt.savefig('fig3.png', dpi=200)\nplt.scatter(x, y)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "to_graph2 = []\ncharacters = pro_corpus.get_usernames()\nfor character in characters:\n    user1 = series_corpus.get_user(character)\n    utterances = len(user1.get_utterance_ids())\n    if utterances < 500:\n        to_graph2.append((character, pro_corpus.get_user(character).meta[\"utterance_per_conversation\"], utterances))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x = [i[1] for i in to_graph2]\ny = [i[2] for i in to_graph2]\nplt.xlabel('Utterances Per Conversation')\nplt.ylabel('Utterances')\nplt.title('Utterances to Share of Conversation(Major Removed)')\nplt.savefig('fig4.png', dpi=200)\nplt.scatter(x, y)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "to_graph3 = []\ncharacters = pro_corpus.get_usernames()\nfor character in characters:\n    user1 = series_corpus.get_user(character)\n    utterances = len(user1.get_utterance_ids())\n    to_graph3.append((character, pro_corpus.get_user(character).meta[\"first_last_word\"], utterances))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x = [i[1] for i in to_graph3]\ny = [i[2] for i in to_graph3]\nplt.savefig('fig5.png', dpi=200)\nplt.scatter(x, y)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "to_graph3 = []\ncharacters = pro_corpus.get_usernames()\nfor character in characters:\n    user1 = series_corpus.get_user(character)\n    utterances = len(user1.get_utterance_ids())\n    if utterances < 500:\n        to_graph3.append((character, pro_corpus.get_user(character).meta[\"first_last_word\"], utterances))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x = [i[1] for i in to_graph3]\ny = [i[2] for i in to_graph3]\nplt.savefig('fig6.png', dpi=200)\nplt.scatter(x, y)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}